{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning\n",
      "\n",
      "Top 14 Simple LTV Customers:\n",
      "   Simple LTV   customer_id        amt\n",
      " 12870.000000  00535c7d8f42  24.750000\n",
      " 10639.571429  00435c7d8f42  20.460714\n",
      " 10277.800000  01135c7d8f42  19.765000\n",
      " 10034.440000  00635c7d8f42  19.297000\n",
      "  9392.240000  01035c7d8f42  18.062000\n",
      "  8653.840000  01235c7d8f42  16.642000\n",
      "  8340.800000  00835c7d8f42  16.040000\n",
      "  8290.608696  00735c7d8f42  15.943478\n",
      "  8201.200000  96f55c7d8f42  15.771538\n",
      "  8027.500000  00935c7d8f42  15.437500\n",
      "  7324.488889  00135c7d8f42  14.085556\n",
      "  7309.342857  00235c7d8f42  14.056429\n",
      "  6654.581818  00335c7d8f42  12.797273\n",
      "  2900.000000  200000000001   5.576923\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "import os.path\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import exc\n",
    "\n",
    "'''\n",
    "I used postgresql for windows for my Data warehouse: postgresql-13.0-1-windows-x64.exe\n",
    "\n",
    "After installing postgress with: password    for user postgress password:\n",
    "\n",
    "1.\n",
    "createdb -h localhost -p 5432 -U postgres bobtest\n",
    "password: password  (was specified when prompted)\n",
    "\n",
    "2.\n",
    "createuser -P -U postgres bob \n",
    "\n",
    "3.\n",
    "C:\\Program Files\\PostgreSQL\\13\\bin>psql bobtest postgres\n",
    "Password for user postgres: password  --(entered)\n",
    "\n",
    "4.\n",
    "bobtest=# \\du                         --(\\du shows users)\n",
    "results:\n",
    " Role name |                         Attributes                         | Member of\n",
    "-----------+------------------------------------------------------------+-----------\n",
    " bob       |                                                            | {}\n",
    " postgres  | Superuser, Create role, Create DB, Replication, Bypass RLS | {}\n",
    "5.\n",
    "bobtest=# exit\n",
    "\n",
    "Please contact me if you need any help or have any questions re postgres install\n",
    "\n",
    "''' \n",
    "\n",
    "global debug,exists,engine\n",
    "debug = False #True\n",
    "exists = 'append'\n",
    "engine = create_engine('postgresql://bob:password@localhost:5432/bobtest') # Database and credentials\n",
    "\n",
    "def CUSTOMER_NEW(df,D):\n",
    "#   CUSTOMER, NEW\n",
    "    if debug: print('Inside CUSTOMER_NEW ')    \n",
    "\n",
    "    df1=df.iloc[[0] , :2 ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    df2=df.iloc[[0], 2:7]\n",
    "    df2.to_sql('CUSTOMER', D, if_exists=exists, index=False)\n",
    "    if len(df.index) == 1: return\n",
    "\n",
    "    df1=df.iloc[[1] , : ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    tags_str = str(df1['tags'].squeeze())\n",
    "    df2=df.iloc[[1], np.r_[2,3,7]]\n",
    "    df2['tags'] = tags_str.replace('1 ','') # adding string to 'tags' (since df2=df.iloc[[1], np.r_[2,3,7,8]] fails)\n",
    "    if 'SITE_VISIT' == type and 'NEW' == verb:\n",
    "        df2.to_sql('SITE_VISIT', D, if_exists=exists, index=False)\n",
    "\n",
    "    df1=df.iloc[[2] , :2 ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    df2=df.iloc[[2], np.r_[2,3,7,9,10]]\n",
    "    if 'IMAGE' == type and 'UPLOAD' == verb:\n",
    "        df2.to_sql('IMAGE', D, if_exists=exists, index=False)\n",
    "\n",
    "    df1=df.iloc[[3] , :2 ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    df2=df.iloc[[3], np.r_[2,3,7,11]]\n",
    "    if 'ORDER' == type and 'NEW' == verb:\n",
    "        df2.to_sql('ORDER', D, if_exists=exists, index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def CUSTOMER_UPDATE(df,D):\n",
    "#   CUSTOMER, UPDATE            Update these relative fields:  [4,5,6,7]\n",
    "    if debug: print('Inside CUSTOMER_UPDATE 1 ')\n",
    "\n",
    "    df1=df.iloc[[0] , : ]\n",
    "\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    key  = df1['key'].squeeze()\n",
    "    event_time = df1['event_time'].squeeze()\n",
    "    last_name =df1['last_name'].squeeze()\n",
    "    adr_city =df1['adr_city'].squeeze()\n",
    "    adr_state =df1['adr_state'].squeeze()\n",
    "    \n",
    "    sql_update1 = 'UPDATE public.\"CUSTOMER\" SET event_time = '   \n",
    "    sql_update2 = \"'\" + str(event_time) + \"' ,last_name = '\" + last_name\n",
    "    sql_update3 = \"' ,adr_city = '\" + adr_city\n",
    "    sql_update4 = \"' ,adr_state = '\" + adr_state + \"' WHERE key = '\" + key + \"';\"\n",
    "    sql_update  = sql_update1 + sql_update2 +  sql_update3 + sql_update4\n",
    "\n",
    "    sqlcmd(sql_update)\n",
    "    return\n",
    "\n",
    "\n",
    "def IMAGE_NEW(df,D):\n",
    "#   IMAGE, NEW\n",
    "    if debug: print('Inside IMAGE_NEW ')\n",
    "\n",
    "    df1=df.iloc[[0] , :2 ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    df2=df.iloc[[0], np.r_[2,3,4,5,6]]\n",
    "    df2.to_sql('IMAGE', D, if_exists=exists, index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def ORDER_NEW(df,D):\n",
    "#   ORDER, NEW\n",
    "    if debug: print('Inside ORDER_NEW ')\n",
    "\n",
    "    df1=df.iloc[[0] , :2 ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    df2=df.iloc[[0], np.r_[2,3,4,5]]\n",
    "    df2.to_sql('ORDER', D, if_exists=exists, index=False)\n",
    "    return\n",
    "\n",
    "\n",
    "def ORDER_UPDATE(df,D):\n",
    "#   ORDER, UPDATE               Update these relative fields:  [3,4,6]\n",
    "    if debug: print('Inside ORDER_UPDATE ')\n",
    "\n",
    "    df1=df.iloc[[0] , : ]\n",
    "\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    key  = df1['key'].squeeze()\n",
    "    event_time = df1['event_time'].squeeze()\n",
    "    customer_id = df1['customer_id'].squeeze()\n",
    "    total_amount = df1['total_amount'].squeeze()\n",
    "\n",
    "    sql_update1 = 'UPDATE public.\"ORDER\" SET event_time= '\n",
    "    sql_update2 = \"'\" + str(event_time)\n",
    "    sql_update3 = \"' ,total_amount = '\" + total_amount\n",
    "    sql_update4 = \"' WHERE key ='\" + key + \"' AND  customer_id = '\" + customer_id + \"'\"\n",
    "    sql_update  = sql_update1 + sql_update2 +  sql_update3 + sql_update4\n",
    "\n",
    "    sqlcmd(sql_update)\n",
    "    return\n",
    "\n",
    "\n",
    "def SITE_VISIT_NEW(df,D):\n",
    "#   SITE_VISIT, NEW\n",
    "    if debug: print('Inside SITE_VISIT_NEW ')\n",
    "\n",
    "    df1=df.iloc[[0] , : ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "    tags_str = str(df1['tags'].squeeze())\n",
    "    df2=df.iloc[[0], np.r_[2,3,4]]\n",
    "    df2['tags'] = tags_str.replace('0 ','') # adding string to 'tags' (since df2=df.iloc[[1], np.r_[2,3,7,8]] fails)\n",
    "    df2.to_sql('SITE_VISIT', D, if_exists=exists, index=False)\n",
    "    return\n",
    "\n",
    "# Ingest record e, D is Database connection\n",
    "def Ingest(e,D):\n",
    "    if debug: print('Inside Ingest Records ')\n",
    "\n",
    "    df = pd.read_json(e)\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    df1=df.iloc[[0] , :2 ]\n",
    "    type = df1['type'].squeeze()\n",
    "    verb = df1['verb'].squeeze()\n",
    "\n",
    "# Determine What file types and verbs are found, then process each file found \n",
    "    if 'CUSTOMER'   == type and 'NEW' == verb:    CUSTOMER_NEW(df,D)\n",
    "    if 'SITE_VISIT' == type and 'NEW' == verb:    SITE_VISIT_NEW(df,D)\n",
    "    if 'IMAGE'      == type and 'UPLOAD' == verb: IMAGE_NEW(df,D)\n",
    "    if 'ORDER'      == type and 'NEW' == verb:    ORDER_NEW(df,D)\n",
    "\n",
    "    if 'CUSTOMER'   == type and 'UPDATE' == verb: CUSTOMER_UPDATE(df,D)   #[4,5,6,7]\n",
    "    if 'ORDER'      == type and 'UPDATE' == verb: ORDER_UPDATE(df,D)      #[3,4,6]\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def sqlcmd(input):      #process update statements\n",
    "    if debug:print('Inside sqlcmd')\n",
    "    SQLAlchemyError = 'Dummy'   \n",
    "    try:\n",
    "        with engine.connect() as con:\n",
    "           rs = con.execute(input)\n",
    "    except SQLAlchemyError as e:\n",
    "        error = str(e.__dict__['orig'])\n",
    "        return error\n",
    "     \n",
    "    return\n",
    "\n",
    "\n",
    "# Find Top Simple LTV Customers (x), D is Database connection\n",
    "def TopXSimpleLTVCustomers(x, D):    \n",
    "    if debug: print('Inside TopXSimpleLTVCCustomers')\n",
    "    utc=pytz.UTC\n",
    "    \n",
    "    query = '''\n",
    "    select * from public.\"ORDER\"\n",
    "    '''\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "\n",
    "#  Split total_amount int amt and cur()    \n",
    "    df[['amt','cur']] = df.total_amount.str.split(\" \",expand=True) \n",
    "    df['amt'] = pd.to_numeric(df['amt'])\n",
    "    \n",
    "#   Find the week of year\n",
    "    year_week = []\n",
    "    for t in df['event_time']:\n",
    "        week = str(t.isocalendar()[1])\n",
    "        if len(week) == 1:  week = '0' + str(week)\n",
    "        year_week_row_entry = str(t.year) + '-' + week\n",
    "        year_week.append(year_week_row_entry)\n",
    "   \n",
    "    df['year_week'] = year_week    \n",
    "    df['amt'] = pd.to_numeric(df['amt'])   \n",
    "\n",
    "    \n",
    "# Find the Sum of the values found in a week    \n",
    "# identify the columns we want to aggregate by; this could\n",
    "    group_cols = ['customer_id','year_week']\n",
    "# identify the columns which we want to average; this could\n",
    "    metric_cols = [ 'amt']\n",
    "# create a new DataFrame with a MultiIndex consisting of the group_cols\n",
    "# and a column for the mean of each column in metric_cols\n",
    "    aggs = df.groupby(group_cols)[metric_cols].sum()\n",
    "# remove the metric_cols from df because we are going to replace them\n",
    "# with the means in aggs\n",
    "    df.drop(metric_cols, axis=1, inplace=True)\n",
    "    df.drop(['total_amount','cur'], axis=1, inplace=True)  \n",
    "# dedupe to leave only one row with each combination of group_cols in df\n",
    "    df.drop_duplicates(subset=group_cols, keep='last', inplace=True)\n",
    "# add the mean column from aggs into df\n",
    "    df = df.merge(right=aggs, right_index=True, left_on=group_cols, how='right')\n",
    "\n",
    "    \n",
    "#   Arbitrarily based on my data using last 10 years,   could have calculated it if needed to from the data    \n",
    "    mindate = datetime.strptime('2011-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "    maxdate = datetime.strptime('2021-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')   \n",
    "    mask = (df['event_time'] > mindate.replace(tzinfo=utc)) & (df['event_time'] <= maxdate.replace(tzinfo=utc))\n",
    "    df_filtered=df.loc[mask]\n",
    "\n",
    "    \n",
    "#   Find the means for each week   Similar to Find the Sum above \n",
    "    group_cols = ['customer_id']\n",
    "    metric_cols = [ 'amt']\n",
    "    aggs = df.groupby(group_cols)[metric_cols].mean()\n",
    "    df.drop(metric_cols, axis=1, inplace=True)\n",
    "    df.drop_duplicates(subset=group_cols, keep='last', inplace=True)\n",
    "#   Add the mean column from aggs into df\n",
    "    df = df.merge(right=aggs, right_index=True, left_on=group_cols, how='right')\n",
    "\n",
    "#   Calculate Simple LTV per customer    \n",
    "    df['Simple LTV'] = 52*df['amt']*10\n",
    "\n",
    "#   Sort the result with Simple LTV in descending order with customer_id and amt         \n",
    "    df_sort = df.sort_values(['Simple LTV','customer_id'], ascending=[False,True]).reset_index()\n",
    "    df_simpleLTV =df_sort[['Simple LTV','customer_id', 'amt']]\n",
    " \n",
    "    return df_simpleLTV\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Beginning\\n')\n",
    "\n",
    "#   used to drop all of the tables every time; so job can be run any time    \n",
    "    drop_tables = 'DROP TABLE IF EXISTS public.\"CUSTOMER\", public.\"SITE_VISIT\", public.\"IMAGE\", public.\"ORDER\"'\n",
    "\n",
    "#   used to set up primary and foreign keys    \n",
    "    alter_table =  ['ALTER TABLE public.\"CUSTOMER\" ADD PRIMARY KEY (\"key\")',\n",
    "                   'ALTER TABLE public.\"SITE_VISIT\" ADD PRIMARY KEY (\"key\")',\n",
    "                   'ALTER TABLE public.\"IMAGE\" ADD PRIMARY KEY (\"key\")',\n",
    "                   'ALTER TABLE public.\"ORDER\" ADD PRIMARY KEY (\"key\")',\n",
    "                   'ALTER TABLE public.\"SITE_VISIT\" ADD CONSTRAINT IMAGE_FK FOREIGN KEY(customer_id) REFERENCES public.\"CUSTOMER\"(key)',\n",
    "                   'ALTER TABLE public.\"IMAGE\" ADD CONSTRAINT IMAGE_FK FOREIGN KEY(customer_id) REFERENCES public.\"CUSTOMER\"(key)',\n",
    "                   'ALTER TABLE public.\"ORDER\" ADD CONSTRAINT IMAGE_FK FOREIGN KEY(customer_id) REFERENCES public.\"CUSTOMER\"(key)']\n",
    "\n",
    "    \n",
    "# Cleanup: Drop tables if they exists\n",
    "    sqlcmd(drop_tables)\n",
    "\n",
    "# Process json file with top_x, NEW, UPLOAD AND UPDATE Records\n",
    "    fileName = '1file_input.json'\n",
    "    first = True\n",
    "    count=0\n",
    "    with open(fileName) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            df = pd.read_json(line)\n",
    "            count += 1\n",
    "#           read value for x (top_x variable in json file) for function: TopXSimpleLTVCustomers(x,D)            \n",
    "            if first:\n",
    "                df.reset_index(drop=True)\n",
    "                df1=df.iloc[[0] , :1 ]\n",
    "                x = int(float(df1['top_x'].squeeze()))\n",
    "                first = False\n",
    "                continue          \n",
    "                               \n",
    "            e = line\n",
    "            D = engine\n",
    "            Ingest(e, D)\n",
    "# Close json file           \n",
    "    jsonfile.close()      \n",
    "\n",
    "# Add primary and foreign keys to tables    \n",
    "    for i in alter_table:\n",
    "        sqlcmd (i)\n",
    "    \n",
    "\n",
    "#   find Top x customers,  x from top_x variable in json file     \n",
    "    D = engine\n",
    "    df_top_x_results = TopXSimpleLTVCustomers(x, D)\n",
    "    \n",
    "    y = x    #only used for the following print statement \n",
    "    print('Top ' + str(y) + ' Simple LTV Customers:')\n",
    "    df_topx = df_top_x_results.head(x)\n",
    "    print(df_topx.to_string(index = False))\n",
    "    \n",
    "    print('\\nFinished')\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
